---
title: "Métodos Cuantitativos II"
subtitle: "Medidas de dependencia central y estacionariedad"
author: 
  - "Alejandro Mosiño - _Universidad de Guanajuato_"
date: "v. 2026.01.15"

knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../docs") })

output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
    transition: 0
    slide_level: 3
    css: ../src/custom.css
---


<a class="qmii-footer" href="QMII--00--Dependencia-Serial.pdf" target="_blank">
  PDF
</a>

## Dependencia serial

### Introducción

Antes de modelar una serie de tiempo, es necesario describir su comportamiento. Para esto calculamos:

1. La función promedio.
2. La función de autocovarianza.
3. La función de autocorrelación.

Veremos en qué consisten estas funciones y algunos ejemplos.



### Función promedio

La **función promedio** se define como:

$$
\mu_{y_t} = \mathbb{E}(y_t)
$$


donde \( y_t \) es una serie de tiempo en el momento \( t \), \(t=1,...,T\).



### Función promedio: Ejemplos (1/2)

Considera una serie de tiempo que se comporta como:

$$
\begin{align}
&y_t = \frac{1}{3}(\varepsilon_{t-1}+\varepsilon_{t}+\varepsilon_{t+1}) \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$

En este caso:


$$
\begin{align}
\mu_{y_t} &= \mathbb{E}(y_t)\\
&=\frac{1}{3}\left[\mathbb{E}(\varepsilon_{t-1}) +\mathbb{E}(\varepsilon_{t}) + \mathbb{E}(\varepsilon_{t+1})\right]\\
&=0
\end{align}
$$

### Función promedio: Ejemplos (2/2)

Considera una serie de tiempo que se comporta como:


$$
\begin{align}
y_t &= \delta + y_{t-1} + \varepsilon_t; \ y_0 = 0\\
&= \delta t + \sum_{j=1}^{t}\varepsilon_j\\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$

En este caso:


$$
\mu_{y_t} = \delta t
$$



### Función de autocovarianza

La **función de autocovarianza** se define como:


$$
\gamma_y (t, \tau) = E[(y_t - \mu_t)(y_\tau - \mu_\tau)]
$$


donde \( y_t \) es alguna serie de tiempo en el momento \( t \). Nota que si \( \tau=t \):

$$
\begin{align}
\gamma(t, t) &=  \mathbb{E}[(y_t - \mu_t)^2]\\
& =\mathbb{V}ar(y_t)
\end{align}
$$


### Función de autocovarianza: Ejemplos (1/2)

Considera una serie de tiempo que se comporta como:


$$
\begin{align}
&y_t = \varepsilon_t \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$

En este caso:

$$
\gamma_y(t, \tau) = \begin{cases}
\sigma^2 & \text{si} \ t=\tau \\
0 & \text{si} \ t\neq\tau
\end{cases}
$$

### Función de autocovarianza: Ejemplos (2/2)

Considera una serie de tiempo que se comporta como:

$$
\begin{align}
&y_t = \frac{1}{3}(\varepsilon_{t-1}+\varepsilon_{t}+\varepsilon_{t+1}) \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$

En este caso:

$$
\gamma_y(t, \tau) = \begin{cases}
\frac{1}{3}\sigma^2 & \text{si} \ |t-\tau|=0 \\
\frac{2}{9}\sigma^2 & \text{si} \ |t-\tau|=1 \\
\frac{1}{9}\sigma^2 & \text{si} \ |t-\tau|=2 \\
0 & \text{si} \ |t-\tau|>2 \\
\end{cases}
$$

### Función de autocorrelación (1/2)

La **función de autocorrelación** se define como:


$$
\rho(t,\tau) = \frac{\gamma_y(t,\tau)}{\sqrt{\gamma_y(t,t) \gamma_y(\tau,\tau)}}
$$


donde \( y_t \) es una serie de tiempo en el momento \( t \).

### Función de autocorrelación (2/2)

Nota que la función de autocorrelación se simplifica cuando la varianza de \(y_t\) es constante. Esto es, si \(\gamma_y(t,t) = \gamma_y(\tau,\tau)\), entonces: 


$$
\rho(t,\tau) = \frac{\gamma_y(t,\tau)}{\gamma_y(t,t)}
$$


Si además \( t=\tau\):

$$
\rho(t,\tau) = 1
$$

### Función de autocorrelación: Ejemplos (1/2)

Considera una serie de tiempo que se comporta como:


$$
\begin{align}
&y_t = \varepsilon_t \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$


En este caso:

$$
\rho_y(t, \tau) = \begin{cases}
1 & \text{si} \ t=\tau \\
0 & \text{si} \ t\neq\tau
\end{cases}
$$

### Función de autocorrelación: Ejemplos (2/2)

Considera una serie de tiempo que se comporta como:


$$
\begin{align}
&y_t = \frac{1}{3}(\varepsilon_{t-1}+\varepsilon_{t}+\varepsilon_{t+1}) \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$


En este caso:

$$
\rho_y(t, \tau) = \begin{cases}
1 & \text{si} \ |t-\tau|=0 \\
\frac{2}{3} & \text{si} \ |t-\tau|=1 \\
\frac{1}{3} & \text{si} \ |t-\tau|=2 \\
0 & \text{si} \ |t-\tau|>2 \\
\end{cases}
$$

### Otras medidas de dependencia serial

Otras medidas de dependencia serial son:


- La **función de covarianza cruzada**:

$$
\gamma_{xy} (t, \tau) = E[(x_t - \mu_{x_t})(y_\tau - \mu_{y_\tau)}]
$$


- La **función de correlación cruzada**:

$$
\rho_{xy} (t, \tau) = \frac{\gamma_{xy}(t,\tau)}{\sqrt{\gamma_x(t,t) \gamma_y(\tau,\tau)}}
$$

## Estacionariedad

### Estacionariedad

El concepto de **estacionariedad** (débil) es importante para modelar una serie de tiempo. Una serie \( y_t \) es estacionaria si:


1. Su media es constante:

   $$
   \mu_{y_t} = \mu_{y} 
   $$


2. Su función de autocovarianza no depende de \( t \) o \( \tau \), sino de \( h = t-\tau \):

   $$
   \gamma_y(t+k, \tau+k) = \gamma_y(t,\tau); \ k \ \in \ \mathbb{Z}
   $$


### Ejemplo de estacionariedad (1/4)

Considera un ruido blanco:


$$
\begin{align}
&y_t = \varepsilon_t \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$


Este proceso es estacionario puesto que:

$$
\begin{align}
E(y_t) &= 0 \\ 
\gamma_y(t,\tau) &= \begin{cases}
\sigma^2 & \text{si} \ t=\tau \ (h=0)\\
0 & \text{si} \ t\neq\tau \ (h\neq0)
\end{cases}
\end{align}
$$

### Ejemplo de estacionariedad (2/4)

Considera una serie de tiempo que se comporta como:


$$
\begin{align}
&y_t = \frac{1}{3}(\varepsilon_{t-1}+\varepsilon_{t}+\varepsilon_{t+1}) \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$


Este proceso es estacionario puesto que:

$$
\begin{align}
E(y_t) &= 0 \\
\gamma_y(t, \tau) &= \begin{cases}
\frac{1}{3}\sigma^2 & \text{si} \ h=0 \\
\frac{2}{9}\sigma^2 & \text{si} \ |h|=1 \\
\frac{1}{9}\sigma^2 & \text{si} \ |h|=2 \\
0 & \text{si} \ |h|>2 \\
\end{cases}
\end{align}
$$


### Ejemplo de estacionariedad (3/4)

Considera un proceso autorregresivo de orden 1:


$$
\begin{align}
&y_t = \phi y_{t-1} + \varepsilon_t \\
&\varepsilon_t \sim iid \mathcal{N}(0,\sigma^2)
\end{align}
$$


Este proceso es estacionario si \( |\phi| < 1 \), ya que:

$$
\begin{align}
E(y_t) &= 0 \\
\gamma_y(t,\tau) &= \begin{cases}
\frac{\sigma^2}{1-\phi^2)}; & |h| = 0 \\
\phi \frac{\sigma^2}{1-\phi^2)}; & |h| = 1 \\
\phi^2 \frac{\sigma^2}{1-\phi^2)}; & |h| = 2 \\
\vdots &\\
\end{cases}
\end{align}
$$

### Ejemplo de estacionariedad (4/4)

Considera una caminata aleatoria:


$$
\begin{align}
y_t &= \delta + y_{t-1} + \varepsilon_t; \ y_0 = 0 ; \ \varepsilon_t \sim iid \mathcal{N}(0,\sigma^2) \\
&= \delta t + \sum_{j=1}^{t}\varepsilon_j
\end{align}
$$


En este caso, ambas condiciones para la estacionariedad se violan, puesto que:

$$
\begin{align}
\mu_{y_t} &= \delta t\\
\gamma_y(t,\tau) &= \sigma^2\min(t,\tau)
\end{align}
$$


### Estacionario vs. No estacionario

¿Cuál(es) de estos procesos es (son) estacionario(s)?


```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='60%', out.height='60%'}

library(astsa)

set.seed(12345)

x=rnorm(500)
y=filter(x, filter=c(1.5,-.75), method="recursive")[-(1:50)]
z=cumsum(x)
culer = c(rgb(.66,.12,.85), rgb(.12,.66,.85), rgb(.85,.30,.12))
tsplot(cbind(WN=x, AR=y, RW=z), type='o', pch=19, col=culer)
dev.off()

```

### Estacionariedad: Función promedio y autocovarianza

De los resultados anteriores, es fácil deducir que, para un proceso estacionario:


1. La función promedio:

   $$
   \begin{align}
   \mu_y &= \mathbb{E}(y_t) \\
   & = \mu
   \end{align}
   $$


2. La función de autocovarianza solo depende del desfase \( h = t-\tau \):

   $$
   \begin{align}
   \gamma_y(t,\tau) &= \gamma_y(t,t-h) \\
   &= \gamma_y(h,0) \\
   &= \gamma_y(h) \\
   &= \gamma_y(-h) \\
   \end{align}
   $$
   
### Descomposición de Wold (1/2)

La siguiente es la **descomposición de Wold**:

Cualquier serie estacionaria \( y_t \) puede reescribirse como una combinación lineal de ruidos blancos:


$$
\begin{align}
y_t &= \mu + \sum_{j=0}^\infty \psi_j \varepsilon_{t-j}, \ \sum_{j=0}^\infty\psi_j^2<\infty; \ \psi_0=1\\
\varepsilon_t &\sim iid \mathcal{N}(0,\sigma^2).
\end{align}
$$

### Descomposición de Wold (2/2)

Entonces, si \( y_t \) es una serie estacionaria:

   $$
   \mathbb{E}(y_t) = \mu.
   $$

Y:

   $$
   \gamma_y(h) = \sigma^2 \sum_{j=0}^\infty \psi_j \psi_{j+h}.
   $$
   
## Dependencia serial: estimación

### Dependencia serial: estimación (1/3)


El cálculo de las funciones promedio, autocovarianza y autocorrelación depende de nuestro conocimiento de los parámetros del modelo y del ruido blanco. En ausencia de estos, podemos usar las observaciones \( y_1, y_2, \dots, y_T \) para estimar:

1. **Promedio muestral:**

   $$
   \hat{\mu} = \frac{1}{T} \sum_{t=1}^T y_t.
   $$

2. **Autocovarianzas muestrales:**

   $$
   \hat{\gamma}(h) = \frac{1}{T} \sum_{t=1}^{T-h} (y_t - \hat{\mu})(y_{t+h} - \hat{\mu}).
   $$

### Dependencia serial: estimación (2/3)


<ol start="3">
  <li>**Autocorrelaciones muestrales:**

   $$
   \hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}.
   $$</li>
  

Se puede demostrar que si la serie \( y_t \) es un ruido blanco, entonces para \( T \) grande y bajo ciertas condiciones, la función de autocorrelación muestral se distribuye aproximadamente como:

$$
\hat{\rho}(h) \sim \mathcal{N}\left(0, \frac{1}{T}\right).
$$

### Dependencia serial: estimación (3/3)

El **correlograma** es la representación gráfica de la función de autocorrelación muestral, incluyendo intervalos de confianza. 

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='60%', out.height='60%'}

par(mfrow=c(3,1))
acf1(x, 48, main="FAC Ruido Blanco")
acf1(y, 48, main="FAC AR(2)")
acf1(z, 48, main="FAC Caminata Aleatoria")
dev.off()

```

### Otras medidas de dependencia serial (1/2)

**Autocovarianza y autocorrelación cruzadas:**

$$
\hat\gamma_{xy}(h) = \frac{1}{T} \sum_{t=1}^{T-h} (x_t - \hat{\mu_x})(y_{t+h} - \hat{\mu_y}).
$$

$$
\hat\rho_{xy}(h) = \frac{\hat\gamma_{xy}(h)}{\sqrt{\hat\gamma_x(0)\hat\gamma_y(0)}}.
$$

Se puede demostrar que si las series \( x_t \) y \( y_t \) son independientes, entonces para \( T \) grande:

$$
\hat{\rho}_{xy}(h) \sim \mathcal{N}\left(0, \frac{1}{T}\right).
$$

### Otras medidas de dependencia serial (2/2)

Algunos ejemplos:

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='60%', out.height='60%'}

par(mfrow=c(3,1))
ccf2(x, y, 48, main="Ruido Blanco & AR(2)")
ccf2(x, z, 48, main="Ruido Blanco & Caminata Aleatoria")
ccf2(y, z, 48, main="AR(2) & Caminata Aleatoria")
dev.off()

```


### Checklist

Al terminar este capítulo, deberías ser capaz de:

- Calcular e interpretar autocovarianzas y autocorrelaciones muestrales.
- Construir e interpretar un correlograma (ACF) como herramienta descriptiva.
- Tener una primera intuición de cómo luce una serie estacionaria (media y varianza estables).

