---
title: "Métodos Cuantitativos II"
subtitle: "Modelos Autorregresivos con Medias Móviles"
author: 
  - "Alejandro Mosiño - _Universidad de Guanajuato_"
date: "v. 2026.02.09"

knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../docs") })

output: 
  ioslides_presentation:
    widescreen: true
    smaller: true
    transition: 0
    slide_level: 3
    css: ../src/custom.css
---

<a class="qmii-footer" href="QMII--02--ARMA.pdf" target="_blank">
  PDF
</a>

## Introducción

### Estacionariedad (1/2)

Recordemos que una serie de tiempo, \( y_t \), es estacionaria si:


1. Su media es constante:

   $$
   \begin{align}
   \mathbb{E}(y_t) &= \mu_{y_t} \\
   & = \mu
   \end{align}
   $$
   
2. Su función de autocovarianza, \( Cov(y_t, y_{t-h}) \),  es tal que:

   $$
   \begin{align}
   Cov(y_t, y_{t-h}) & = \gamma_y(h)  \\
   &= \gamma(h); \ \forall \ h \in \mathbb{Z}
   \end{align}
   $$

### Estacionariedad (2/2)

¿Cuáles de estas series de tiempo parecen estacionarias?

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='70%', out.height='60%'}

library(tidyverse)
library(astsa)
library(here)

culer = c('cyan4', 4, 2, 6)


Quarterly <- read.csv(here("data", "E306--Quarterly.csv"))
Unemp <- ts(Quarterly$Unemp, start =c(1960,1), frequency = 4)
Tbill <- ts(Quarterly$Tbill , start =c(1960,1), frequency = 4)
CPI <- ts(Quarterly$CPI, start =c(1960,1), frequency = 4)
IndProd <- ts(Quarterly$IndProd, start =c(1960,1), frequency = 4)

par(mfrow = c(2,2), cex.main=1)
tsplot(Unemp, type='o', pch=19, col=culer, main="Tasa de desempleo", xlab="Tiempo", ylab="Desempleo")
tsplot(Tbill, type='o', pch=19, col=culer, main= "Bonos del tesoro", xlab="Tiempo", ylab="Rendimiento")
tsplot(CPI, type='o', pch=19, col=culer, main="Precios al consumidor", xlab="Tiempo", ylab="CPI")
tsplot(IndProd, type='o', pch=19, col=culer, main="Producción industrial", xlab="Tiempo", ylab="IPI")
par(mfrow=c(1,1))

```


### Modelos estacionarios

Para modelar series de tiempo estacionarias usamos modelos **Autorregresivos con Promedios Móviles (ARMA)**. Estos combinan:

- _Parte autorregresiva, AR_: La serie se modela como una combinación lineal de sus valores pasados.

- _Parte de medias móviles, MA_: La serie se modela en función de sus errores pasados.
  

Los modelos ARMA nos permiten capturar la **parte cíclica** de una serie de tiempo, es decir, las fluctuaciones que ocurren alrededor de una media constante.

### Modelos AR

Formalmente, un modelo **AR(\(p\))** se define como:

  $$
  \begin{align}
  y_t &= \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t\\
  &= \sum_{i=1}^{p} \phi_i y_{t-i} + \varepsilon_t
  \end{align}
  $$
  
Donde:

  - \( \phi_i \) son los coeficientes autorregresivos, \(i=1,\dots,p\).
  - \( \varepsilon_t \) es un ruido blanco.
  
### Modelos MA

Un modelo **MA(\(q\))** se define como:

  $$
  \begin{align}
  y_t &= \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q} \\
  &= \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}
  \end{align}
  $$
  
Donde:

  - \( \theta_j \) son los coeficientes de promedios móviles, \(j=1,\dots,q\).
  - \( \varepsilon_t \) es un ruido blanco.
  

### Modelos ARMA

Un modelo **ARMA(\(p\),\(q\))** combina los modelos AR(\(p\)) y MA(\(q\)):

  $$
  \begin{align}
  y_t &= \sum_{i=1}^{p} \phi_i y_{t-i} + \varepsilon_t \\
  & + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}
  \end{align}
  $$

Por ejemplo, un modelo ARMA(\(1\),\(1\)):

$$
  y_t = \phi_1 y_{t-1} + \varepsilon_t + \theta_1 \varepsilon_{t-1}
$$


## El operador de rezagos

### El operador de rezagos

El **operador de rezagos**, \(L\) o \(B\), es un _operador lineal_ tal que, aplicado a una serie de tiempo, \(y_t\):

$$
L y_t = y_{t-1}
$$

Naturalmente:

$$
\begin{align}
L^2 y_t &= L(L y_t)\\
&= L y_{t-1} \\
&= y_{t-2}
\end{align}
$$

En general:

$$
L^i y_t = y_{t-i}
$$

### El operador de rezagos y los modelos ARMA (1/2)

Un modelo ARMA(\(p\), \(q\)) puede reescribirse en términos del operador de rezagos. En particular:

$$
  \begin{align}
  y_t = \phi_1 y_{t-1} &+ \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t \\
  & +\theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q} \\
  \end{align}
$$

puede reescribirse como:

$$
  \begin{align}
  y_t = \phi_1 Ly_{t} &+ \phi_2 L^2y_{t} + \dots + \phi_p L^py_{t} + \varepsilon_t \\
  & + \theta_1 L\varepsilon_{t} + \theta_2 L^2\varepsilon_{t} + \dots + \theta_q L^q\varepsilon_{t}
  \end{align}
$$

### El operador de rezagos y los modelos ARMA (2/2)

Un modelo ARMA(\(p\), \(q\)) puede reescribirse en términos del operador de rezagos:

$$
  \Phi(L) y_t = \Theta(L)\varepsilon_{t}
$$

Donde:

$$
\begin{align}
  \Phi(L)  & = 1-\phi_1 L - \phi_2 L^2 - \phi_3 L^3 - \dots - \phi_p L^p\\
  \Theta(L)  &= 1+\theta_1 L + \theta_2 L^2 + \theta_3 L^3 + \dots + \theta_q L^q
  \end{align}
$$

### Estacionariedad e Invertibilidad (1/2)

Usando la notación en términos del operador de rezagos, es posible demostrar que un proceso ARMA(\(p\), \(q\)) es:

1. **Estacionario** si las raíces de \(\Phi(L)=0\) están _fuera_ del círculo unitario (\(|L|>1\)).  
2. **Invertible** si las raíces de \(\Theta(L)=0\) están _fuera_ del círculo unitario (\(|L|>1\)).

<span class="emphasis-blue">Nota</span>  

- Un proceso AR estacionario puede reescribirse como un MA(\(\infty\)).  
- Un proceso MA invertible puede reescribirse como un AR(\(\infty\)).  

Estas condiciones garantizan que el modelo tenga una representación estable y única.


### Estacionariedad e Invertibilidad (2/2)

Aunque en la práctica no trabajamos explícitamente con representaciones AR(\(\infty\)) o MA(\(\infty\)), su existencia es fundamental porque permite:

- Calcular momentos poblacionales (medidas de dependencia serial).
- Construir funciones impulso–respuesta.
- Derivar la _función de autocorrelación parcial_.  
  

## Descomposición de Wold y medidas de dependencia serial

### Descomposición de Wold (1/3)

Consideremos un modelo ARMA(\(p\),\(q\)):

$$
  \Phi(L) y_t = \Theta(L)\varepsilon_{t}\\
  \varepsilon_t \sim WN(0,\sigma^2)
$$

Deseamos calcular la funciones promedio, de autocovarianza y de autocorrelación. Para esto, conviene expresar el proceso en su representación MA(\(\infty\)). Luego, hacemos uso de la **descomposición de Wold**.

### Descomposición de Wold (2/3)

Sea \(\{y_t\}\) un proceso débilmente estacionario con media cero y varianza finita. Entonces existe una representación única:

$$
y_t = \sum_{i=0}^{\infty} \psi_i \varepsilon_{t-i}, 
\qquad \psi_0 = 1, 
\qquad \sum_{i=0}^{\infty} \psi_i^2 < \infty.
$$

donde \(\varepsilon_t\) es un ruido blanco.


### Descomposición de Wold (3/3)

Entonces, si \( y_t \) es una serie estacionaria:

   $$
   \mathbb{E}(y_t) = \mu.
   $$

Y:

   $$
   \gamma_y(h) = \sigma^2 \sum_{j=0}^\infty \psi_j \psi_{j+h}.
   $$


### Descomposición de Wold: MA(\(1\))

Como ejemplo, consideremos un proceso MA(\(1\)):

$$
y_t = \varepsilon_t + \theta_1 \varepsilon_{t-1}
$$

Este proceso cumple naturalmente con la representación de Wold, con:

$$
\psi_0 = 1, \quad \psi_1 = \theta_1.
$$


### Descomposición de Wold: AR(\(1\))

Ahora, consideremos un proceso AR(\(1\)):

$$
y_t = \phi_1 y_{t-1} + \varepsilon_t
$$

Si \(|\phi_1|<1\), el proceso tiene representación MA(\(\infty\)):

$$
y_t = \sum_{i=0}^{\infty} \phi_1^i \varepsilon_{t-i}.
$$

Entonces, en este caso:

$$
\psi_i = \phi_1^i.
$$

### Descomposición de Wold: Ejercicio

Usando la descomposición de Wold, muestra que, para un proceso MA(\(1\)):

$$
\gamma(h) = \begin{cases}
(1 + \theta_1^2) \sigma^2 & h=0\\
\theta_1 \sigma^2 & |h|=1\\
0 & |h|>1
\end{cases}
\qquad
\rho(h) = \begin{cases}
1 & h=0\\
\frac{\theta_1}{1 + \theta_1^2} & |h|=1\\
0 & |h|>1
\end{cases}
$$

Y muestra que, para un proceso AR(\(1\)) con \(|\phi_1|<1\):

$$
\gamma(h)=\phi_1^{|h|}\frac{\sigma^2}{1-\phi_1^2},
\qquad
\rho(h)=\phi_1^{|h|}.
$$

### Función de autocorrelación (FAC): identificación empírica

A nivel empírico, la función de autocorrelación (FAC) permite identificar patrones iniciales en la dinámica de la serie. En particular:


<table class="clean-table">
<thead>
<tr>
<th>Modelo</th>
<th>Patrón FAC</th>
</tr>
</thead>
<tbody>
<tr>
<td>AR(p)</td>
<td>Decae gradualmente</td>
</tr>
<tr>
<td>MA(q)</td>
<td>Se corta después del rezago q</td>
</tr>
<tr>
<td>ARMA(p,q)</td>
<td>Decae gradualmente</td>
</tr>
</tbody>
</table>


La FAC permite detectar la presencia de memoria en la serie y orientar la especificación del modelo.


## La función de impulso respuesta

### Función de Impulso-Respuesta (1/2)

La **Función de Impulso-Respuesta (IRF)**:

- Muestra cómo un choque en el término de error, \( \varepsilon_t \), afecta a una serie de tiempo, \(y_t\), en periodos futuros.

- Permite analizar la _dinámica temporal_ de un proceso y la persistencia de los efectos de un shock.

### Función de Impulso-Respuesta (2/2)

Si el proceso admite representación MA(\(\infty\)):

$$
y_t = \sum_{j=0}^\infty \psi_j \varepsilon_{t-j},
$$

entonces la IRF es simplemente:

$$
IRF(j) = \psi_j.
$$

Esta nos indica el efecto que tiene un shock (_normalizado a una unidad_) en el periodo \(s\)-ésimo, \(\varepsilon_{s}\), sobre la variable \(y_t\).

### Función de Impulso-Respuesta: MA(\(1\))

Consideremos un proceso MA(\(1\)):

  $$
  y_t = \varepsilon_t + \theta_1 \varepsilon_{t-1}
  $$

En este caso, la FIR:

<table class="clean-table">
  <thead>
    <tr>
      <th>Periodo</th>
      <th>\( s-2 \)</th>
      <th>\( s-1 \)</th>
      <th>\( s \)</th>
      <th>\( s+1 \)</th>
      <th>\( s+2 \)</th>
      <th>\( s+3 \)</th>
      <th>\( \dots \)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\( \varepsilon_t \)</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>\( \dots \)</td>
    </tr>
    <tr>
      <td>\( y_t \)</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>\( \theta_1 \)</td>
      <td>0</td>
      <td>0</td>
      <td>\( \dots \)</td>
    </tr>
  </tbody>
</table>



### Función de Impulso-Respuesta: AR(1)

Para calcular la FIR de un proceso AR(\(1\)), primero tenemos que invertirlo. Como hemos visto antes, un proceso AR(\(1\)) puede invertirse en un proceso MA(\(\infty\)) en tanto el coeficiente \(|\phi_1|<1\):

$$
y_t = \sum_{i=0}^{\infty} \phi_1^i \varepsilon_{t-i}
$$

Entonces, la FIR es:

<table class="clean-table">
  <thead>
    <tr>
      <th>Periodo</th>
      <th>\( s-2 \)</th>
      <th>\( s-1 \)</th>
      <th>\( s \)</th>
      <th>\( s+1 \)</th>
      <th>\( s+2 \)</th>
      <th>\( s+3 \)</th>
      <th>\( \dots \)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\( \varepsilon_t \)</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>\( \dots \)</td>
    </tr>
    <tr>
      <td>\( y_t \)</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>\( \phi_1 \)</td>
      <td>\( \phi_1^2 \)</td>
      <td>\( \phi_1^3 \)</td>
      <td>\( \dots \)</td>
    </tr>
  </tbody>
</table>



### Función de Impulso-Respuesta: Ejemplo (1/2)

<span class="emphasis-blue">Ejercicio</span>

Considera el proceso AR(\(2\)):

$$
y_t = 0.6y_{t-1} + 0.2y_{t-2} + \varepsilon_{t}
$$

<ol start="1">
  <li>Muestra que la FIR es:
   </li>
   
   <table class="clean-table">
  <thead>
    <tr>
      <th>Periodo</th>
      <th>\( s-2 \)</th>
      <th>\( s-1 \)</th>
      <th>\( s \)</th>
      <th>\( s+1 \)</th>
      <th>\( s+2 \)</th>
      <th>\( s+3 \)</th>
      <th>\( \dots \)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\( \varepsilon_t \)</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>\( \dots \)</td>
    </tr>
    <tr>
      <td>\( y_t \)</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.6</td>
      <td>0.516</td>
      <td>0.4579</td>
      <td>\( \dots \)</td>
    </tr>
  </tbody>
</table>

### Función de Impulso-Respuesta: Ejemplo (2/2)

<span class="emphasis-blue">Ejercicio</span>

<ol start="2">
<li>Grafica la FIR de este proceso. Muestra que esta es:
</li>

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='60%', out.height='60%'}

# Parámetros del modelo AR(2)
phi1 <- 0.6
phi2 <- 0.2
num_periods <- 20  # Número de periodos a calcular

# Inicializar la FIR con un choque unitario en t=0
impulse_response <- numeric(num_periods)
impulse_response[1] <- 1  # Choque inicial en epsilon_t

# Calcular la FIR iterativamente
for (t in 2:num_periods) {
  if (t == 2) {
    impulse_response[t] <- phi1 * impulse_response[t-1]
  } else {
    impulse_response[t] <- phi1 * impulse_response[t-1] + phi2 * impulse_response[t-2]
  }
}

# Graficar la función de impulso-respuesta
plot(0:(num_periods-1), impulse_response, type="h", lwd=2, col="cyan4",
     xlab="Periodos", ylab="FIR",
     main="Función de Impulso-Respuesta del AR(2)")
points(0:(num_periods-1), impulse_response, pch=16, col="red")
grid()

```


## Función de autocorrelación parcial

### La función de autocorrelación parcial (1/4)

Considera un proceso AR(\(p\)):

$$
\begin{align}
y_t &= \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t \\
\varepsilon_t &\sim WN(0,\sigma^2)
\end{align}
$$

Este puede reescribirse como:

$$
\begin{align}
y_t &= \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + 0\cdot y_{t-p-1} + 0\cdot y_{t-p-2} + 0\cdot y_{t-p-m} + \varepsilon_t \\
\varepsilon_t &\sim WN(0,\sigma^2)
\end{align}
$$


### La función de autocorrelación parcial (2/4)

Lo anterior indica que, _si nuestro proceso es realmente un AR(\(p\))_ todos los coeficientes:

$$
\begin{align}
\phi_{p+1} &= \phi_{p+2} = \dots = \phi_{p+m} \\
&= 0
\end{align}
$$
En palabras: 1) la FACP es distinta de cero hasta el rezago \(p\); 2) para rezagos mayores, la FACP es cero.


Esta observación da origen a la definición de **función de autocorrelación parcial**.

### La función de autocorrelación parcial (3/4)

La función de autocorrelación parcial (FACP), no es más que la secuencia de coeficientes \(\phi_{kk}\) del sistema:

$$
\begin{align}
y_t &= \phi_{11}y_{t-1} + \varepsilon_{1t}\\
y_t &= \phi_{21}y_{t-1} + \phi_{22}y_{t-2} + \varepsilon_{2t}\\
y_t &= \phi_{31}y_{t-1} + \phi_{32}y_{t-2} + \phi_{33}y_{t-3} + \varepsilon_{3t}\\
&\vdots\\
y_t &= \phi_{k1}y_{t-1} + \phi_{k2}y_{t-2} + \dots + \phi_{kk}y_{t-k} + \varepsilon_{kt}\\
\end{align}
$$

La FACP mide la correlación entre \(y_t\) y \(y_{t-k}\) una vez controlado el efecto lineal de los rezagos intermedios.


### La función de autocorrelación parcial (4/4)

Para encontrar la FACP podemos utilizar la siguiente fórmula de _correlación condicional_:

$$
\text{Corr}(X,Y\mid Z)=\frac{\rho_{XY}-\rho_{XZ}\rho_{YZ}}
{\sqrt{(1-\rho_{XZ}^2)(1-\rho_{YZ}^2)}}.
$$


### La función de autocorrelación parcial: Ejemplos

Podemos mostrar que, para un proceso AR(\(1\)), la función de autocorrelación parcial es:

$$
\begin{align}
\phi_{11} &= \rho_1 \\
\end{align}
$$


Y, para un proceso AR(\(2\)):

$$
\begin{align}
\phi_{11} &= \rho_1 \\
\phi_{22} &= \frac{\rho_2-\rho_1^2}{1-\rho_1^2}
\end{align}
$$

### La función de autocorrelación parcial: Estimación

En la práctica, la FACP no es más que una secuencia de coeficientes estimados por el método de los _mínimos cuadrados ordinarios_. 



### La función de autocorrelación parcial: identificación empírica

Al igual que la FAC, la función de autocorrelación parcial permite identificar patrones iniciales en la dinámica de la serie. En este caso tenemos:

<table class="clean-table">
<thead>
<tr>
<th>Modelo</th>
<th>Patrón FACP</th>
</tr>
</thead>
<tbody>
<tr>
<td>AR(p)</td>
<td>Se corta después del rezago p</td>
</tr>
<tr>
<td>MA(q)</td>
<td>Decae gradualmente</td>
</tr>
<tr>
<td>ARMA(p,q)</td>
<td>Decae gradualmente</td>
</tr>
</tbody>
</table>


## Identificación del orden de un ARMA(\(p\),\(q\))

### Identificación del orden de un ARMA(\(p\),\(q\)) (1/8)

En ausencia de parámetros,usamos las observaciones \(\{y_t\}_{t=1}^T\) para encontrar:

  - Promedio muestral
  - Autocovarianza muestral
  - Autocorrelación muestral
  - Autocorrelación parcial
  
Para identificar el orden de un ARMA(\(p\),\(q\)), _comparamos el comportamiento de las medidas de dependencia serial muestrales con el comportamiento teórico_ para diferentes valores de \(p\) y de \(q\).

### Identificación del orden de un ARMA(\(p\),\(q\)) (2/8)

<span class="emphasis-blue">Regla 1</span>

La _función de autocorrelación_ puede ayudarnos a saber si el modelo adecuado es un AR/ARMA o un MA. Si el modelo es un MA, la función de autocorrelación nos indica su orden.

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='70%', out.height='60%'}

# Cargamos librerías (si no las tienes instaladas, usa install.packages())
library(astsa)

# Semilla para reproducibilidad
set.seed(123)

# Número de observaciones en la simulación
n <- 200

# Vector de colores
culer <- c("cyan4", 4, 2, 6)

# Simulaciones de cada proceso:
sim_ma1    <- arima.sim(model = list(order = c(0,0,1), ma = 0.5),         n = n)
sim_ma2    <- arima.sim(model = list(order = c(0,0,2), ma = c(0.5, -0.3)),n = n)
sim_ar1    <- arima.sim(model = list(order = c(1,0,0), ar = 0.7),         n = n)
sim_ar2 <- arima.sim(model = list(order = c(2,0,0), ar = c(0.2,0.7)), n = n)

# Ajustamos la ventana de 2x2 y configuramos márgenes y tamaños de texto
par(
  mfrow    = c(2,2),      # 2 filas, 2 columnas
  mar      = c(4,4,3,1),  # márgenes (abajo, izq, arriba, der)
  cex.main = 1.3,         # tamaño del título
  cex.lab  = 1.2,         # tamaño de las etiquetas de ejes
  cex.axis = 1.1          # tamaño de las marcas en los ejes
)

# Gráfico 1: FAC de MA(1)
acf(sim_ma1, 
    main = "FAC de MA(1)",
    col  = culer,
    lwd  = 2)             # grosor de línea

# Gráfico 2: FAC de MA(2)
acf(sim_ma2,
    main = "FAC de MA(2)",
    col  = culer,
    lwd  = 2)

# Gráfico 3: FAC de AR(1)
acf(sim_ar1,
    main = "FAC de AR(1)",
    col  = culer,
    lwd  = 2)

# Gráfico 4: FACP de AR(2)
acf(sim_ar2,
    main = "FAC de AR(2)",
    col  = culer,
    lwd  = 2)

dev.off()
```

Si la FAC presenta corte abrupto, tenemos evidencia de componente MA(\(q\)).

### Identificación del orden de un ARMA(\(p\),\(q\)) (3/8)

<span class="emphasis-blue">Regla 2</span>

Si el modelo resulta ser un AR(\(p\)), la _función de autocorrelación parcial_ nos indicará su orden.


```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='70%', out.height='60%'}


par(
  mfrow    = c(2,2),      # 2 filas, 2 columnas
  mar      = c(4,4,3,1),  # márgenes (abajo, izq, arriba, der)
  cex.main = 1.3,         # tamaño del título
  cex.lab  = 1.2,         # tamaño de las etiquetas de ejes
  cex.axis = 1.1          # tamaño de las marcas en los ejes
)

# Gráfico 1: FACP de MA(1)
pacf(sim_ma1, 
    main = "FACP de MA(1)",
    col  = culer,
    lwd  = 2)             # grosor de línea

# Gráfico 2: FACP de MA(2)
pacf(sim_ma2,
    main = "FACP de MA(2)",
    col  = culer,
    lwd  = 2)

# Gráfico 3: FACP de AR(1)
pacf(sim_ar1,
    main = "FACP de AR(1)",
    col  = culer,
    lwd  = 2)

# Gráfico 4: FACP de AR(2)
pacf(sim_ar2,
    main = "FACP de ARMA(2)",
    col  = culer,
    lwd  = 2)

dev.off()
```

Si la FACP presenta corte abrupto, tenemos evidencia de componente AR(\(p\)).


### Identificación del orden de un ARMA(\(p\),\(q\)) (4/8)

<span class="emphasis-blue">Regla 3</span>

Los residuales del modelo seleccionado deben comportarse (estadísticamente) como un ruido blanco.


### Identificación del orden de un ARMA(\(p\),\(q\)) (5/8)

Para saber si el residual de un proceso se comporta (estadísticamente) como un ruido blanco, usamos el **estadístico Q** de _Ljung-Box_:

$$
Q_{LB} = T(T+2)\sum_{h=1}^m\frac{\rho_h^2}{T-h}
$$

Probamos la hipótesis nula:

$$
\begin{align}
H_0 :& \ \varepsilon_t \sim WN \\
H_1 :& \ \varepsilon_t \nsim WN
\end{align}
$$

Si la hipótesis nula es correcta, se puede demostrar que:

$$
Q_{LB} \sim \chi^2_m; \ m \approx \sqrt{T}
$$

### Identificación del orden de un ARMA(\(p\),\(q\)) (6/8)

<span class="emphasis-blue">Regla 4</span>

Si tenemos más de un modelo, podemos seleccionar el mejor usando la función de verosimilitud. En general, el modelo con un _valor más alto_ para la función de (log) verosimilitud es el mejor.

Esto también nos ayuda a elegir el orden de un proceso ARMA(\(p,q\)) cuando \(p\) y \(q\) son simultáneamente diferentes de cero.


### Identificación del orden de un ARMA(\(p\),\(q\)) (7/8)


Alternativamente, podemos elegir el modelo comparando:

- Los valores del **criterio de información de Akaike** (AIC):

$$
AIC = \mathbb{E}[-2\mathcal{L}(\cdot)] = T\ln(s^2) + 2k
$$

- Los valores del **criterio de información de Schwartz** (SIC o BIC):

$$
BIC = T\ln(s^2) + k\ln(T)
$$

En general, buscamos el modelo que tenga el _menor valor posible_ para estos criterios.


### Identificación del orden de un ARMA(\(p\),\(q\)) (8/8)


<span class="emphasis-blue">Regla 5</span>

Si el AIC y el BIC seleccionan modelos diferentes, tomar en cuenta que:

- _AIC es eficiente_: selecciona el modelo que tenga el error de predicción más pequeño.

- _BIC es consistente_: selecciona el modelo correcto con probabilidad 1 si \(T\) es grande.

### Estimación de un ARMA(\(p\),\(q\))


<span class="emphasis-blue">Nota</span>

Una vez que hemos identificado el orden del proceso, podemos estimar este usando el **método de máxima verosimilitud**.


## Pronósticos

### Pronósticos usando ARMA(\(p\),\(q\))

Una vez que hemos estimado los coeficientes de un proceso ARMA(\(p\),\(q\)), podemos utilizar el modelo para realizar pronósticos. 

Sea \(y_T\) el último valor disponible de \(y_t\). Entonces, el mejor pronóstico para \(h\) periodos hacia el futuro de \(y_t\), \(\widehat y_{T+h\mid T}, \ h=1,2,\dots \), es la **proyección lineal óptima**:

$$
\begin{align}
\widehat y_{T+h\mid T} &= \mathbb{E}(y_{T+h}\mid \Omega_T),\\ 
\Omega_T&=\{y_T,y_{T-1},y_{T-2},\dots\}
\end{align}
$$
la cual, _en el caso de normalidad_, coincide con el valor esperado condicional.

### Pronósticos usando ARMA(\(p\),\(q\)): Ejemplo

<span class="emphasis-blue">Ejemplo</span>

Consideremos un proceso AR(\(1\)):

$$
\begin{align}
y_t&=\phi_0+\phi_1y_{t-1}+\varepsilon_t, \ |\phi_1|<1\\
\varepsilon_t & \sim iid\mathcal{N}(0,\sigma^2)\\
\end{align}
$$
En este caso tenemos:

$$
\begin{align}
\mathbb{E}(y_{T+1}\mid\Omega_T) &= \phi_0 + \phi_1 y_T,\\
\mathbb{E}(y_{T+2}\mid\Omega_T) &= \phi_0 (1+\phi_1) + \phi_1^{2} y_T,\\
\mathbb{E}(y_{T+3}\mid\Omega_T) &= \phi_0(1+\phi_1+\phi_1^{2}) + \phi_1^{3} y_T,\\
& \vdots \\
\mathbb{E}(y_{T+h}\mid\Omega_T) &= \phi_0\sum_{j=0}^{h-1}\phi_1^{j} + \phi_1^{h} y_T\\
&= \mu(1-\phi_1^h)+\phi_1^h y_T \\
\end{align}
$$

### Pronósticos usando ARMA(\(p\),\(q\)) (3/3)

<span class="emphasis-blue">Ejemplo (cont'd)</span>

Nota que, para un proceso AR(\(1\)), si consideramos un horizonte más largo:

$$
\lim_{h\to\infty}\mathbb{E}(y_{T+h}\mid\Omega_T)=\mu,
$$

donde:

$$
\mu=\frac{\phi_0}{1-\phi_1}.
$$
Esto es, \(\mathbb{E}(y_{T+h})\) _converge a la media no condicional_ de \(y_t\).


### Varianza del pronóstico (1/2)

Por supuesto, la calidad del pronóstico \(y_{T+h}\) depende crucialmente de la **varianza del error del pronóstico**, \(\mathbb{V}ar\big[y_{T+h}-\mathbb{E}(y_{T+h}\mid\Omega_T)\big]\).


<span class="emphasis-blue">Ejemplo</span>

Consideremos un proceso AR(\(1\)). La varianza del error de pronóstico de \(y_t\) para \(h\) periodos hacia el futuro es:

$$
\begin{align}
\mathbb{V}ar\!\big[y_{T+h}-\mathbb{E}(y_{T+h}\mid\Omega_T)\big]
&=\sigma^2\sum_{j=0}^{h-1}\phi_1^{2j} \\
&=\sigma^2\frac{1-\phi_1^{2h}}{1-\phi_1^2}
\end{align}
$$



### Varianza del pronóstico (2/2)

Nota que, conforme \(h\) crece, tenemos que la varianza del pronóstico _converge a la varianza no condicional_ de \(y_t\):

$$
\lim_{h\to\infty}\mathbb{V}ar\!\big[y_{T+h}-\mathbb{E}(y_{T+h}\mid\Omega_T)\big] =  
\frac{\sigma^2}{1-\phi_1^2}.
$$

### Pronósticos, caso general

Recordemos que, si un proceso ARMA(\(p,q\)) es estacionario e invertible, existe una representación MA(\(\infty\)) de la forma:

$$
y_t=\mu + \sum_{i=0}^{\infty}\psi_i\,\varepsilon_{t-i},\qquad \psi_0=1.
$$

En este caso:

$$
\begin{align}
\mathbb{V}ar\!\big[y_{T+h}-\mathbb{E}(y_{T+h}\mid\Omega_T)\big]
&=\sigma^2\sum_{i=0}^{h-1}\psi_i^{2} \\
\lim_{h\to\infty}\mathbb{E}(y_{T+h}\mid\Omega_T)&=\mu\ .
\end{align}
$$

### Intervalos de predicción

Si \(\varepsilon_t\sim iid \mathcal{N}(0,\sigma^2)\), un intervalo de confianza de \(100(1-\alpha)\%\) para \(y_{T+h}\) se calcula como:

$$
\widehat y_{T+h\mid T}\ \pm\ z_{1-\alpha/2}\,\sqrt{\mathbb{V}ar\!\big[y_{T+h}-\mathbb{E}(y_{T+h}\mid\Omega_T)\big]}.
$$


Por ejemplo, si \(\alpha=0.05\), \(z_{1-\alpha/2}=1.96\). Entonces:

$$
\widehat y_{T+h\mid T}\ \pm\ 1.96\,\sqrt{\mathbb{V}ar\!\big[y_{T+h}-\mathbb{E}(y_{T+h}\mid\Omega_T)\big]}.
$$


### Estimación y pronósticos: Ejemplo

En el siguiente ejemplo usamos un índice para el empleo en Canadá que va desde el primer trimestre de 1962 al cuarto trimestre de 1992. Usamos un modelo AR(\(2\)) para pronosticar los cuatro trimestres de 1993 y 1994.



```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE, out.width='70%', out.height='60%'}

library(here)
library(astsa)

employment <- read.csv(here("data", "E311--Employment.csv"))
caemp <- ts(employment$caemp, start=c(1961,1), end=c(1994,4), frequency = 4)
train <- window(caemp, end=c(1992,4))
h <- length(window(caemp, start=c(1993,1)))
sarima.for(train, n.ahead=h, p=2, d=0, q=0,
           main="Empleo en Canadá: ajuste AR(2) y pronóstico",
           xlab="Tiempo", ylab="Índice de empleo")

lines(caemp, lwd=2)

dev.off()

```
